Описание файлов проекта, .env и инструкция по запуску

**Структура проекта:**

```
├── build_index.py         # Построение FAISS индекса из JSON
├── config.py              # Загрузка переменных окружения и конфигов
├── data_processor.py      # Преобразование JSON в документы для индексации
├── faiss_index/           # Каталог с индексом FAISS (index.faiss, index.pkl)
├── itmo_programs_full.json# Основной JSON с данными о программах
├── llm_system.py          # Основная логика RAG и работы с LLM
├── pdf_debug.txt          # Временный файл для отладки парсинга PDF
├── pyproject.toml         # Метаинформация проекта (если используется poetry)
├── requirements.txt       # Список зависимостей Python
├── run_full_pipeline.py   # Полный запуск: скрапинг, индекс, бот (не рекомендуется для продакшена)
├── scraper.py             # Парсер сайтов и учебных планов (HTML+PDF)
├── telegram_bot.py        # Telegram-бот на aiogram
├── .env.example           # Пример файла переменных окружения
├── .gitignore             # Исключения для git
└── ...
```

**.env.example** — пример файла для переменных окружения:

```
# DeepSeek API ключ
DEEPSEEK_API_KEY=sk-your-deepseek-api-key-here
# Telegram Bot Token
TELEGRAM_BOT_TOKEN=your-telegram-bot-token-here
# Настройки для DeepSeek
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
# Модель для эмбеддингов и извлечения
EMBEDDINGS_MODEL=paraphrase-multilingual-mpnet-base-v2
RETRIEVER_K=5
# Файл с данными
JSON_DATA_FILE=itmo_programs_full.json
# Логирование
LOG_LEVEL=INFO
```

**Инструкция по запуску (рекомендуемый порядок):**

1. Сначала выполните парсинг данных:
    ```bash
    python scraper.py
    ```
    Это создаст/обновит файл itmo_programs_full.json с актуальными данными.

2. Затем постройте векторный индекс:
    ```bash
    python build_index.py
    ```
    Это создаст папку faiss_index/ с индексом для быстрого поиска.

3. После этого можно запускать Telegram-бота:
    ```bash
    python telegram_bot.py
    ```

> **Важно:**  Есть возможность запуска через run_full_pipeline.py, но это неоптимально - оптимальнее выполнять этапы по отдельности, чтобы каждый раз не запускать процесс парсинга и индексации векторного хранилища.

---
Добрый день, сейчас я расскажу ход своих мыслей при решении задачи с разработкой ТГ Бота.

1. Парсер (scraper.py)

    Для того, чтобы его написать, я внимательно проанализировал страницу программ на направлениях, и извлек из нее общую структуру, которая работает для всех программ. Для того, чтобы быть уверенным, я подгрузил в контекстное окно LLM всю страницу, и она помогла мне гарантированно извлечь нужную информацию - нашла все нужные элементы в коде. Для парсинга HTML я использовал библиотеку BeautifulSoap, потому, что мне уже приходилось с ней работать и синтаксис был знаком, а для поулчения данных с сайтов через GET запрос я использовал библиотеку requests.

    Дальше был реализован парсер учебных планов. Парсер скачивает PDF-файл учебного плана, извлекает текст постранично и разбивает его на строки. Затем он определяет тип дисциплины по заголовкам секций, ищет семестры (в том числе поддерживает несколько семестров через запятую), название дисциплины и кредиты, связывая их между собой. Для каждой найденной дисциплины формируется запись с названием, семестром, типом и количеством кредитов. В конце парсер фильтрует дубликаты и сохраняет итоговую структуру в JSON. 

2. Хранение информации. 

    Так как планировал использовать LangChain, то решил взять векторное хранилище из этой библиотеки - использовал FAISS, так как я уже использовл его для написания RAG систем и знаю, как с ним работать. После загрузки парсинга данных из JSON, я использую функционал из файлов build_index.py и data_processor.py, которые строят по извлеченным данным векторное хранилище. В качестве EMBEDDINGS_MODEL я использовал paraphrase-multilingual-mpnet-base-v2. У меня была мысль взять какой-то более мощный инструмент, вроде Lightrag, но это решение справляется с задачей и требует меньше ресурсов, чем LightRag.

3. ТГ бот

    Тг бот реализован на python-библиотеке aiogram, это надежная асинхронная библиотека которая идеально подходит под наши задачи.


4. Модуль с LLM (llm_system.py)

- Для каждого вопроса пользователя:
    1. Классифицирует вопрос с помощью LLM (тип: general, specific, recommendation), передавая 3 последние пары "вопрос-ответ" и последнюю обсуждаемую программу.
    2. Определяет, к какой программе относится вопрос (по явному упоминанию или по истории).
    3. Выполняет поиск релевантных документов в FAISS-векторном хранилище (по нужной программе или по всем).
    4. Генерирует финальный ответ с помощью LLM на основе найденных документов.
    5. Сохраняет историю диалога для каждого пользователя.

Такой подход позволяет учитывать контекст диалога, уточняющие вопросы и персонализировать ответы.

#### Схема работы RAGSystem

```
Пользователь в Telegram
          |
          v
Telegram-бот (aiogram)
          |
          v
    process_message
          |
          v
    RAGSystem.ask
          |
          v
-----------------------------
| Классификация вопроса                    |
| (LLM + 3 предыдущие пары "вопрос-ответ") |
-----------------------------
          |
    +----+-------------------+
    |                        |
    v                        v
specific/recommendation   general
    |                        |
    v                        v
Определение программы     Поиск по всем программам
(явно/по истории)             |
    |                        v
    v                  [FAISS поиск]
[FAISS поиск]               |
    |                        |
    +-----------+------------+
                    |
                    v
          Генерация ответа (LLM)
                    |
                    v
          Сохранение в историю
                    |
                    v
            Ответ пользователю
                    |
                    +-------> (возврат к Пользователю)
```

5. Обоснование выбора LLM

    В качестве LLM было принято решение использовать DeepSeek, потому что у меня был ключ и возможность использовать его API, а поднимать на своем компьютере другую LLM локально мне не хотелось из-за отсутствия времени.



